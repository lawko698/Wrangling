{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Intellectual Property Organization: Data Wrangling\n",
    "\n",
    "Date: 01/04/2017\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.6.0 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* nltk (for RegexpTokenizer, MWETokenizer, Stopwords, Collocations, Probability, downloaded from [import nltk nltk.download()])\n",
    "* re (for regular expression, included in Anaconda Python 3.6.0) \n",
    "* sklearn (for CountVectorizer, included in Anaconda Python 3.6.0)\n",
    "* BeautifulSoup (xml parser, included in Anaconda Python 3.6.0)\n",
    "\n",
    "## 1. Introduction\n",
    "The following tasks as required by:\n",
    "#### Task 1: \n",
    "<b>Extract the hierarchical IPC code</b>. World Intellectual Property Organization has a hierarchical classification scheme that contains Section, Class, Subclass, Main Group, and Subgroup. Extract the hierarchical IPC codes for all the patents, and store them in a file, called “classification.txt”,  in the following format:  patent’s_ID:Section,Class,Subclass,Main_group,Subgroup.\n",
    "\n",
    "#### Task 2:\n",
    "<b>Extract the citation network</b>. Each patent cites a number of existing granted patents. Extract all the references for each patent, and store them in a file, called “citations.txt”, in the following format: citing_patent_id:cited_patent_id,cited_patent_id,\n",
    "\n",
    "#### Task 3:\n",
    "<b>Extract and preprocess abstracts</b>. \n",
    "1. We are required to extract all the abstracts for all the patents, and then process and store those abstracts as sparse count vectors. The output file, called “count_vectors.txt”,Each row corresponds to a patent’s abstract, starting with patent_id, followed by “word_index:count” pairs.\n",
    "\n",
    "2. Generate Vocab list called “vocab.txt”. Format of list is 'word_index:vocab'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import process_time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.collocations import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examining and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Examine \"patents.xml\" to determine the content and format, then load it into Python. Explain your finds here.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I examined the patents.xml two ways:\n",
    "1. By text editor\n",
    "2. BeautifulSoup xml parser using prettify()\n",
    "\n",
    "By inspection with text editor, it is revealed many xml documents are connected together in the file. Each xml document have a xml header <?xml version=\"1.0\" encoding=\"utf-8\"?>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "#Count the number of xml documents (by xml header) in the file.\n",
    "header=0\n",
    "file = open('patents.xml','r')\n",
    "for line in file:\n",
    "    if line.startswith(\"<?xml \"):\n",
    "        header += 1      \n",
    "print(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the 2500 patents are individually stored in each xml document. Each document will contain the patent's section, class, subclass, main group, and subgroup within their respective tags. It also contains a number of citations related to the patent, and one abstract describing the patent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use parser to inspect the patents.xml file\n",
    "soup = BeautifulSoup(open('patents.xml'),'lxml-xml')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patents are located in the (doc-number) tag under (us-bibliographic-data-grant) >> (publication-reference) >> (document-id). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, BeautifulSoup stops parsing once it reaches the next document header as shown above. This means the concatenated xml documents must be broken down in some sort of way in order to extract all of the information in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parsing XML and Extracting all the required information \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve the problem of concatenated xml documents, beautiful soup can parse the xml document in html mode. This prevents the parser from stopping after reading one document. It will loop over each document and store patent id, section, class, subclass, main group and subgroup. It will write these information into the required format into classification.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_task1(xml_document):\n",
    "    \"\"\"\n",
    "    Parse through the xml document using beautiful soup. Use html mode to obtain a collection of documents\n",
    "    param: xml_document\n",
    "    return: text document according to task 1\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(xml_document))\n",
    "    root = soup.find_all('us-patent-grant')\n",
    "    output_file = open(\"classification.txt\",\"a\")\n",
    "\n",
    "    for document in root:\n",
    "        finalStr = \"\"\n",
    "        data = []\n",
    "        pat_id = document.find('publication-reference').find('doc-number').contents[0]\n",
    "        data.append(document.find('class').contents[0])\n",
    "        data.append(document.find('subclass').contents[0])\n",
    "        data.append(document.find('main-group').contents[0])\n",
    "        data.append(document.find('subgroup').contents[0])\n",
    "        string = \",\".join(str(i) for i in data)\n",
    "        finalStr = \"{}:{}\\n\".format(pat_id,string)\n",
    "        output_file.write(finalStr) #write into txt file\n",
    "\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below run the code to create the file for task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function parse_xml_task1 took 67.8750000000000000000000000 seconds\n"
     ]
    }
   ],
   "source": [
    "# create classification text file\n",
    "writefile = open(\"classification.txt\",\"w\")\n",
    "writefile.close()\n",
    "\n",
    "start = process_time()\n",
    "parse_xml_task1('patents.xml')\n",
    "end = process_time()\n",
    "print(\"The function {} took {:8.25f} seconds\".format(parse_xml_task1.__name__, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the top 10 lines of classification.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PP021722:01,H,5,00\\n', 'RE042159:01,B,7,14\\n', 'RE042170:06,F,11,00\\n', '07891018:41,D,13,00\\n', '07891019:41,D,13,00\\n', '07891020:41,D,13,00\\n', '07891021:62,B,17,00\\n', '07891023:41,F,19,00\\n', '07891025:61,F,9,02\\n', '07891026:41,D,13,00\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(\"classification.txt\") as file:\n",
    "    head = [next(file) for x in range(10)]\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates the citation.txt as required by task 2.\n",
    "\n",
    "This function is similar to classification function. The only difference is it finds all of the citations in the documents and stores it before formatting and writing into the citation.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_task2(xml_document):\n",
    "    \"\"\"\n",
    "    Parse through the xml document using beautiful soup. Use html mode to obtain a collection of documents\n",
    "    param: xml_document\n",
    "    return: text document according to task 2\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(xml_document))\n",
    "    root = soup.find_all('us-patent-grant')\n",
    "    output_file = open(\"citation.txt\",\"a\")\n",
    "\n",
    "    for document in root:\n",
    "        finalStr = \"\"\n",
    "        citation_list = []\n",
    "        \n",
    "        pat_id = document.find('publication-reference').find('doc-number').contents[0]\n",
    "        citations = document.find('us-bibliographic-data-grant').findAll('citation') \n",
    "        \n",
    "        for citation in citations:\n",
    "            citation_list.append(citation.find('doc-number').contents[0]) #stores the citated patents id\n",
    "        \n",
    "        string = \",\".join(str(i) for i in citation_list) #joins all citations together seperated by comma\n",
    "        finalStr = \"{}:{}\\n\".format(pat_id,string)\n",
    "        output_file.write(finalStr) #write into txt file\n",
    "\n",
    "    output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below run the code to create the file for task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function parse_xml_task2 took 70.7031250000000000000000000 seconds\n"
     ]
    }
   ],
   "source": [
    "# create citation text file\n",
    "writefile = open(\"citation.txt\",\"w\")\n",
    "writefile.close()\n",
    "\n",
    "start = process_time()\n",
    "parse_xml_task2('patents.xml')\n",
    "end = process_time()\n",
    "print(\"The function {} took {:8.25f} seconds\".format(parse_xml_task2.__name__, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the top 10 lines of citation.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PP021722:PP17672,PP18482,PP18483\\n', 'RE042159:4954776,4956606,5015948,5115193,5180978,5332966,5332996,5351003,5381090,5521496,5914593\\n', 'RE042170:3988719,4206996,4803623,4905098,5012281,5161222,5172244,5253152,5263153,5270775,5301262,5341363,5355490,5410754,5537626,5559958,5574859,5580177,5611046,5647056,5828864\\n', '07891018:4561124,4831666,4920577,5105473,5134726,D338281,5611081,5729832,5845333,6115838,6332224,6805957,7089598\\n', '07891019:4355632,4702235,5032705,5148002,5603648,6439942,6757916,6910229\\n', '07891020:4599609,4734072,4843014,5061636,5493730,5635909,6080690,6267232,6388422,6767509,2003/0214408,2004/0009729,197 49 862,101 55 935,203 08 642,103 11 185,103 50 869,103 57 193,WO 00/62633,WO 2004/073798\\n', '07891021:4507808,4627112,4864655,5010591,5031242,5165110,5410759\\n', '07891023:770761,1335927,1398962,1446948,1839143,1852030,1983636,2133505,2411724,2682669,3167786,3401857,4923105,5214806,5319806,5413262,5488738,5497923,5611079,5623735,6021528,6088831,6216931,6766532,6804834,6959455,7318542,D581633,7596813,2002/0112275,2003/0110550,2006/0185056,2006/0289585,2009/0070915\\n', '07891025:4290673,5018223,5689834,6009564,6611966,6772448,1782784,1661534,2002-505157,WO 2007/085001\\n', '07891026:1660342,1669085,1757640,1803335,1857850,1944127,2629094,3484868,4807301,5161257,5168576,5365610,5405312,5423087,5497511,5572737,5689836,5717997,6070273,6408446,6532599,6654960,6804832,7412731,2002/0042944,2006/0059609\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(\"citation.txt\") as file:\n",
    "    head = [next(file) for x in range(10)]\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing the abstracts \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "In order to obtain the abstracts, we use the similar method as the previous two tasks. This time, after obtaining the abstract in the document I would normalize it into lower case, then produce a list of tokens to be stored as values within a dictionary, according to patent ids(keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I have tokenized the abstract by using regular expression which only allow words with a single or mutiple hyphens. (i.e. walking-beam and analog-to-digital are allowed). No numbers will be included since we are only interested in the vocabulary in the abstracts. Also, abbreviations are not included. \n",
    "\n",
    "I will assume words with hyphens are considered to be a unigram (i.e. walking-beam and analog-to-digital are unigrams).\n",
    "\n",
    "According to the task, I will be generating at least 100 bigrams. Then filter the tokens with stopwords, top-20 most frequent words based on word’s document frequency, and words only appearing in one abstract.\n",
    "\n",
    "Finally, I will produce the count_vector.txt and vocab.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_task3(xml_document):\n",
    "    \"\"\"\n",
    "    Parse through the xml document using beautiful soup. Use html mode to obtain a collection of documents\n",
    "    param: xml_document\n",
    "    return: text document according to task 3\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(open(xml_document))\n",
    "    root = soup.find_all('us-patent-grant')\n",
    "    tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-]?[a-zA-Z]+)+\", gaps=False) # keeps only words and hyphenated words\n",
    "    \n",
    "    for document in root:\n",
    "        abstract_list = \"\"\n",
    "        \n",
    "        pat_id = document.find('publication-reference').find('doc-number').contents[0]\n",
    "           \n",
    "        # Certain documents may have multiple paragraphs in an abstract \n",
    "        for abstract in document.abstract.p.contents:\n",
    "            abstract_list = abstract_list + \" \" + abstract.lower() #normalize the abstract words into lower cases\n",
    "     \n",
    "        tokens = tokenizer.tokenize(abstract_list) #tokenize the abstract\n",
    "\n",
    "        abstract_dic[pat_id] = list(tokens) #store the tokens into abstract dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below run the code to create produce abstract_dic which contains patent id and abstract tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The function parse_xml_task3 took 75.5937500000000000000000000 seconds\n"
     ]
    }
   ],
   "source": [
    "abstract_dic = {} #create empty dictionary for key (patent ids) and values (abstract tokens)\n",
    "\n",
    "start = process_time()\n",
    "parse_xml_task3('patents.xml')\n",
    "end = process_time()\n",
    "print(\"The function {} took {:8.25f} seconds\".format(parse_xml_task3.__name__, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct the bigrams, I will need to obtain a list of words from all the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for value in abstract_dic.values():\n",
    "    for word in value:\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I do POS tagging on the words and keep words that are nouns (NN) and adjectives (JJ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_word = []\n",
    "tagged_words = []\n",
    "tagged_word = nltk.pos_tag(words)\n",
    "tagged_words = [word for word in tagged_word if (word[1]=='NN' or word[1]=='JJ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams will be generated from the tagged words with the nltk.collocations.BigramAssocMeasures().\n",
    "\n",
    "The loop operation below will generate bigrams, first, by ignoring bigrams that appear less than 40 times in the tagged words list. A bigram list will store the top 15 bigrams according to PMI score. The next loop, the finder.apply_freq_filter() will be reduced by one (to 39), then store the next top 15 bigrams according to PMI score that is not already in the bigrams list. \n",
    "\n",
    "Once the operation obtains at least 130 bigrams it will stop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = []\n",
    "i = 0\n",
    "while len(bigram) < 130:\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(tagged_words) \n",
    "    g = 40 - i\n",
    "    finder.apply_freq_filter(g) #ignore bigrams to do not appear more than g\n",
    "    j=0\n",
    "    while j <15:\n",
    "        store = finder.nbest(bigram_measures.pmi, 15) #generate top 15 bigrams based on PMI score\n",
    "        save = (store[j][0][0],store[j][1][0])\n",
    "        if save not in bigram and store[j][0][0] != store[j][1][0]:\n",
    "            bigram.append(save)\n",
    "        j+=1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By scanning through the bigram list, there are a few bigrams that do not make sense. Therefore I removed them from the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "discard = [('left', 'right'),('red', 'green'),('float', 'fraction'),('green', 'blue'),\n",
    "           ('water-swellable', 'water-insoluble'),('embarkation', 'disembarkation'),('attributed', 'categorical')]\n",
    "bigrams = [word for word in bigram if word not in discard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add those generated bigrams into the abstract_dic by using MWETokenizer. Words in a bigram will be separated by a underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_bigrams = {}\n",
    "tokenizer = MWETokenizer(bigrams, separator='_')\n",
    "for key,values in abstract_dic.items():\n",
    "    abstract_bigrams[key] = tokenizer.tokenize(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the built-in stopwords from nltk to filter the abstract tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "abstract_stop = {}\n",
    "for key in abstract_bigrams.keys():\n",
    "    abstract_stop[key] = [word for word in abstract_bigrams[key] if word not in stopwords_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate the top 20 words by document frequency. \n",
    "Firstly, by using the set function on the values for each document in the abstract_stop dictionary.\n",
    "This generates unique words in each documents and store it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_top20list = []\n",
    "for value in abstract_stop.values():\n",
    "    set_top20list.extend(list(set(value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can obtain the top 20 words that occur in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd = FreqDist(set_top20list)\n",
    "top20 = fd.most_common(20) #extract words from top 20 words tuples\n",
    "fd.plot(20, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['includes', 'one', 'first', 'second', 'method', 'provided', 'system', 'least', 'device', 'plurality', 'portion', 'apparatus', 'surface', 'including', 'may', 'connected', 'formed', 'also', 'data', 'control']\n"
     ]
    }
   ],
   "source": [
    "top20list = list([top20[x][0] for x in range(0,20)])\n",
    "print(top20list) #list of top 20 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the top 20 words from abstract_stop dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_removedtop20 = {}\n",
    "for key in abstract_stop.keys():\n",
    "    abstract_removedtop20[key] = [word for word in abstract_stop[key] if word not in top20list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find words only appearing in one abstract. Use the hapaxes(). Then we filter the abstract_removedtop20 dictionary.\n",
    "This is the final step before we generate the count vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word = list(fd.hapaxes()) # list of words that occur only in one docoment's abstract\n",
    "abstract_complete = {}\n",
    "for key in abstract_removedtop20.keys():\n",
    "    abstract_complete[key] = [word for word in abstract_removedtop20[key] if word not in one_word]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the count vector using CountVectorizer. \n",
    "The token_patten's regular expression is the same as the tokenizer but also allows bigrams to be included.\n",
    "After constructing the matrix. We obtain 2500 rows which corresponds to the number of documents/patent ids and a total of 5654 column/words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 5654)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", token_pattern=u'(?u)[a-zA-Z]+(?:[-_]?[a-zA-Z]+)+')\n",
    "data_features = vectorizer.fit_transform([' '.join(value) for value in abstract_complete.values()])\n",
    "print(data_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a vocab list by get_feature_names(). It takes all of the 5654 vocabulary as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the Vocab.txt. In the matrix, the vocab are arranged alphabetically with the first column starting with index of 0. Therefore, I just need to add ascending numbers down the vocab list. Please refer to [1] regarding the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"vocab.txt\", \"w\")\n",
    "for word,i in zip(vocab,range(0,len(vocab))):\n",
    "    output_file.write(str(i) + \":\" + word +\"\\n\") # index:word format\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct the count vector, each vocabulary in the document has to be indexed to the vocab.txt. This is done by passing the words into vocab.index(). The index and word count pairs are recorded into store list, which is formatted and written into the count_vector.txt. Please refer to [2] regarding a portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "store=[]\n",
    "vocab = list(vocab)\n",
    "output_file = open(\"count_vector.txt\", 'w') #make a new count_vector text file\n",
    "\n",
    "for key,value in abstract_complete.items(): \n",
    "    i = [vocab.index(word) for word in value] #index the words in the document according to the vocab list\n",
    "    for k, v in FreqDist(i).items(): #pass index list to freqdist\n",
    "        string = (\"{}:{}\".format(k,v)) # produce word_index:count format for a particular document\n",
    "        store.append(string) \n",
    "    \n",
    "    string = \",\".join(i for i in store)\n",
    "    finalStr = \"{},{}\\n\".format(key,string)\n",
    "    store=[]\n",
    "    output_file.write(finalStr) \n",
    "    \n",
    "output_file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "* Task 1. Produces a classification.txt\n",
    "* Task 2. Produces a citation.txt\n",
    "* Task 3. Produces a Vocab.txt and count_vectors.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
